# app.py
# ì‹¤í–‰: streamlit run app.py

import io
import gc
from datetime import timedelta
import numpy as np
import pandas as pd
import streamlit as st
import altair as alt
import matplotlib.pyplot as plt
import seaborn as sns

# Altair í–‰ìˆ˜ ì œí•œ í•´ì œ(í° ë°ì´í„° ê²½ê³  ì™„í™”)
alt.data_transformers.disable_max_rows()

# =========================
# 0) ê¸°ë³¸ ì„¤ì •
# =========================
SESSION_GAP_MIN = 30  # (ë°ì´í„°ì— session_idê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
BIG_FILE_MB = 120     # ì´ í¬ê¸° ì´ìƒì´ë©´ Lite ëª¨ë“œ ê¸°ë³¸ ON
DEFAULT_SAMPLE_FRAC = 0.35

st.set_page_config(page_title="RARRA Dashboard", layout="wide")
st.title("ğŸ“Š RARRA Dashboard (Retention â€¢ Activation â€¢ Referral â€¢ Revenue â€¢ Acquisition)")
st.caption("Kaggle GA Customer Revenue Dataset | ì„¸ì…˜ ë‹¨ìœ„ ë¶„ì„ (ë©”ëª¨ë¦¬ ìµœì í™”)")

# =========================
# 1) ë°ì´í„° ë¡œë”© & ì„¸ì…˜ í…Œì´ë¸” êµ¬ì„±
# =========================
WANTED_COLS = [
    "event_time","fullVisitorId","session_id",
    "totals_pageviews","totals_hits","totals_bounces","totals_newVisits",
    "totals_transactionRevenue","visitNumber","session_duration","is_transaction",
    "channelGrouping","device_deviceCategory",
    "trafficSource_source","trafficSource_medium","trafficSource_referralPath",
    "geoNetwork_continent","device_operatingSystem","date",
]

def _read_csv_safely(file_or_bytes, usecols=None):
    """ì—…ë¡œë”/ê²½ë¡œ ëª¨ë‘ ì§€ì›."""
    if isinstance(file_or_bytes, (str, bytes, io.BytesIO)):
        return pd.read_csv(file_or_bytes, encoding="utf-8-sig", dtype=str,
                           low_memory=False, usecols=usecols)
    # Streamlit UploadedFile
    raw = file_or_bytes.getvalue()
    return pd.read_csv(io.BytesIO(raw), encoding="utf-8-sig", dtype=str,
                       low_memory=False, usecols=usecols)

@st.cache_data(show_spinner=True)
def load_df_memory_smart(file_or_bytes, use_lite: bool, sample_frac: float) -> pd.DataFrame:
    # í—¤ë”ë§Œ ì½ì–´ í•„ìˆ˜ í™•ì¸ + usecols í™•ì •
    if hasattr(file_or_bytes, "getvalue"):
        head = pd.read_csv(io.BytesIO(file_or_bytes.getvalue()), nrows=0, dtype=str)
    else:
        head = pd.read_csv(file_or_bytes, nrows=0, dtype=str)
    required = ["event_time", "fullVisitorId"]
    if not all(c in head.columns for c in required):
        missing = [c for c in required if c not in head.columns]
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

    usecols = [c for c in head.columns if c in WANTED_COLS]
    df = _read_csv_safely(file_or_bytes, usecols=usecols)

    # Lite ìƒ˜í”Œë§
    if use_lite and 0 < sample_frac < 1.0:
        rng = np.random.RandomState(42)
        df = df.loc[rng.rand(len(df)) < sample_frac].copy()

    # ì‹œê°„ ì²˜ë¦¬
    df["event_time"] = pd.to_datetime(df["event_time"], utc=True, errors="coerce")
    df["event_time_naive"] = df["event_time"].dt.tz_convert("UTC").dt.tz_localize(None)

    # ìˆ«ìí˜• ë³€í™˜ + downcast
    def to_num(cols):
        for c in cols:
            if c in df.columns:
                s = pd.to_numeric(df[c], errors="coerce").fillna(0)
                if pd.api.types.is_float_dtype(s):
                    df[c] = pd.to_numeric(s, downcast="float")
                else:
                    df[c] = pd.to_numeric(s, downcast="integer")
    to_num(["totals_pageviews","totals_hits","totals_bounces","totals_newVisits",
            "totals_transactionRevenue","visitNumber","session_duration","is_transaction"])

    # ë¬¸ìì—´ ê¸°ë³¸ê°’
    df["fullVisitorId"] = df["fullVisitorId"].astype(str)
    for c in ["channelGrouping","device_deviceCategory","trafficSource_source",
              "trafficSource_medium","trafficSource_referralPath",
              "geoNetwork_continent","device_operatingSystem","date"]:
        if c not in df.columns:
            df[c] = "Unknown"
        else:
            df[c] = df[c].fillna("Unknown")

    # session_id ìƒì„±(ì—†ë‹¤ë©´ 30ë¶„ ë£°)
    if "session_id" not in df.columns:
        t = df[["fullVisitorId","event_time_naive"]].sort_values(["fullVisitorId","event_time_naive"]).reset_index(drop=True)
        diff = t.groupby("fullVisitorId")["event_time_naive"].diff().dt.total_seconds()
        new_sess = (diff.isna()) | (diff > SESSION_GAP_MIN*60)
        sess_num = new_sess.groupby(t["fullVisitorId"]).cumsum().astype("int32")
        df = df.loc[t.index].copy()
        df["session_id"] = df["fullVisitorId"] + "_" + sess_num.astype(str)

    # ìì£¼ ì“°ëŠ” ë¬¸ìì—´ â†’ category (ë©”ëª¨ë¦¬ ì ˆê°)
    for c in ["channelGrouping","device_deviceCategory","trafficSource_source",
              "trafficSource_medium","trafficSource_referralPath",
              "geoNetwork_continent","device_operatingSystem",
              "session_id","fullVisitorId"]:
        if c in df.columns:
            df[c] = df[c].astype("category")

    return df

@st.cache_data(show_spinner=False)
def build_session_table(df: pd.DataFrame) -> pd.DataFrame:
    sess = (df.groupby("session_id", as_index=False)
              .agg(
                  fullVisitorId=("fullVisitorId","first"),
                  session_start=("event_time_naive","min"),
                  session_end=("event_time_naive","max"),
                  pv=("totals_pageviews","sum"),
                  hits=("totals_hits","sum"),
                  bounces=("totals_bounces","max"),
                  newVisit=("totals_newVisits","max"),
                  visitNumber=("visitNumber","max"),
                  revenue=("totals_transactionRevenue","sum"),
                  channel=("channelGrouping","first"),
                  device=("device_deviceCategory","first"),
                  session_duration=("session_duration","max"),
                  source=("trafficSource_source","first"),
                  medium=("trafficSource_medium","first"),
                  referral_path=("trafficSource_referralPath","first"),
              ))

    # ê²°ì¸¡/í˜• ë³´ì •
    for c in ["pv","hits","bounces","visitNumber","session_duration","revenue"]:
        if c in sess.columns:
            sess[c] = pd.to_numeric(sess[c], errors="coerce").fillna(0)

    for c in ["source","medium","referral_path"]:
        if c in sess.columns:
            sess[c] = sess[c].astype(str).fillna("Unknown")

    # íŒŒìƒ
    sess["session_date"] = pd.to_datetime(sess["session_start"]).dt.date
    sess["session_hour"] = pd.to_datetime(sess["session_start"]).dt.hour
    sess["first_week"] = pd.to_datetime(sess["session_start"]).dt.to_period("W")
    sess["is_transaction"] = (sess["revenue"] > 0).astype(int)

    # 30ì¼ ë‚´ ì¬ë°©ë¬¸ ëˆ„ì  ì¹´ìš´íŠ¸
    s = sess.sort_values(["fullVisitorId","session_start"]).copy()
    first = s.groupby("fullVisitorId")["session_start"].transform("min")
    s["within_30d"] = (s["session_start"] > first) & (s["session_start"] <= first + pd.Timedelta(days=30))
    s["revisit_count_30d"] = s.groupby("fullVisitorId")["within_30d"].cumsum()
    sess = sess.merge(s[["session_id","revisit_count_30d"]], on="session_id", how="left")
    sess["revisit_count_30d"] = sess["revisit_count_30d"].fillna(0).astype(int)

    # ì²« ë°©ë¬¸ ë¼ë²¨
    first_idx = (sess.sort_values(["fullVisitorId","session_start"])
                      .groupby("fullVisitorId", as_index=False).head(1))
    sess["first_channel"] = sess["fullVisitorId"].map(dict(zip(first_idx["fullVisitorId"], first_idx["channel"])))
    sess["first_device"]  = sess["fullVisitorId"].map(dict(zip(first_idx["fullVisitorId"], first_idx["device"])))
    return sess

# =========================
# ì—…ë¡œë” + Lite ì˜µì…˜
# =========================
with st.sidebar:
    st.header("ë°ì´í„° ì—…ë¡œë“œ")
    up = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (utf-8/utf-8-sig ê¶Œì¥)", type=["csv"])
    st.caption("í•„ìˆ˜ ì»¬ëŸ¼: event_time, fullVisitorId")

if up is None:
    st.info("ì™¼ìª½ì—ì„œ CSVë¥¼ ì—…ë¡œë“œí•˜ë©´ ëŒ€ì‹œë³´ë“œê°€ ìƒì„±ë©ë‹ˆë‹¤.")
    st.stop()

file_size_mb = getattr(up, "size", 0) / (1024 * 1024)
with st.sidebar:
    st.markdown("---")
    st.subheader("ë©”ëª¨ë¦¬ ì„¸ì´í”„ ì˜µì…˜")
    lite_default = file_size_mb >= BIG_FILE_MB
    use_lite = st.checkbox("Lite ëª¨ë“œ(ëŒ€ìš©ëŸ‰ ê¶Œì¥)", value=lite_default,
                           help="í•„ìš” ì»¬ëŸ¼ë§Œ ë¡œë“œ, ìˆ«ì downcast, ë¬¸ìì—´ category, ìƒ˜í”Œë§ ì ìš©")
    sample_frac = st.slider("ìƒ˜í”Œ ë¹„ìœ¨", 0.05, 1.0,
                            DEFAULT_SAMPLE_FRAC if lite_default else 1.0, 0.05)
    st.caption(f"ì—…ë¡œë“œ í¬ê¸°: ~{file_size_mb:.1f} MB")

# ì‹¤ì œ ë¡œë“œ
df = load_df_memory_smart(up, use_lite=use_lite, sample_frac=sample_frac)
sess = build_session_table(df)

# ë©”ëª¨ë¦¬ íšŒìˆ˜(ì›ë³¸ dfëŠ” í•„ìš”í•œ ìµœì†Œë§Œ ìœ ì§€)
keep_cols_df = [c for c in ["session_id","geoNetwork_continent","device_operatingSystem",
                            "channelGrouping","device_deviceCategory",
                            "totals_pageviews","totals_newVisits","date"] if c in df.columns]
df = df[keep_cols_df].copy()
gc.collect()

# 30ì¼ ì´ë‚´ ì¬ë°©ë¬¸ ì—¬ë¶€(ì‚¬ìš©ì ë‹¨ìœ„)
def label_user_revisit_30d(sess: pd.DataFrame) -> pd.Series:
    s = sess.copy()
    first = s.groupby("fullVisitorId")["session_start"].transform("min")
    s["within_30d"] = (s["session_start"] > first) & (s["session_start"] <= first + pd.Timedelta(days=30))
    return s.groupby("fullVisitorId")["within_30d"].any().rename("revisit_30d")

# 7ì¼ ì´ë™í‰ê· 
def moving_avg(series: pd.Series, k: int = 7) -> pd.Series:
    return series.rolling(k, min_periods=1).mean()

# =========================
# 2) ì‚¬ì´ë“œë°” í•„í„°
# =========================
min_d = sess["session_date"].min()
max_d = sess["session_date"].max()

with st.sidebar:
    st.markdown("---")
    st.header("í•„í„°")
    start_d = st.date_input("ì‹œì‘ì¼", min_d, min_value=min_d, max_value=max_d)
    end_d   = st.date_input("ì¢…ë£Œì¼", max_d, min_value=min_d, max_value=max_d)
    channels = sorted(sess["channel"].dropna().unique().tolist())
    devices  = sorted(sess["device"].dropna().unique().tolist())
    sel_channels = st.multiselect("ì±„ë„", channels, default=channels)
    sel_devices  = st.multiselect("ë””ë°”ì´ìŠ¤", devices, default=devices)
    min_day_sessions = st.number_input("ì¼ë³„ í‘œë³¸ ìµœì†Œ ì„¸ì…˜ìˆ˜(ìŠ¤íŒŒì´í¬ ê°€ë“œë ˆì¼)", 0, value=500, step=50)

# í•„í„° ì ìš©
mask = (
    (sess["session_date"] >= start_d) &
    (sess["session_date"] <= end_d) &
    (sess["channel"].isin(sel_channels)) &
    (sess["device"].isin(sel_devices))
)
sf = sess.loc[mask].copy()

# --- Activation í”Œë˜ê·¸ ê³„ì‚° (ì„¸ì…˜ ë‹¨ìœ„, ê³ ì • ê¸°ì¤€) ---
sf["act_pageviews"] = (sf["pv"] >= 3).astype(int)
sf["act_duration"]  = (sf["session_duration"] >= 180).astype(int)
sf["act_nonbounce"] = (sf["bounces"] == 0).astype(int)
sf["act_hits"]      = (sf["hits"] >= 10).astype(int)
sf["act_revisit"]   = (sf["revisit_count_30d"] >= 2).astype(int)
sf["activation"] = (
    (sf["act_pageviews"] == 1) &
    (sf["act_duration"]  == 1) &
    (sf["act_nonbounce"] == 1) &
    (sf["act_hits"]      == 1) &
    (sf["act_revisit"]   == 1)
).astype(int)

# ìƒë‹¨ KPI
k1,k2,k3,k4,k5,k6 = st.columns(6)
with k1: st.metric("ì‚¬ìš©ì ìˆ˜", f"{sf['fullVisitorId'].nunique():,}")
with k2: st.metric("ì„¸ì…˜ ìˆ˜", f"{sf['session_id'].nunique():,}")
with k3: st.metric("í‰ê·  PV/ì„¸ì…˜", f"{sf['pv'].mean():.2f}")
with k4: st.metric("Bounce Rate", f"{sf['bounces'].mean():.2%}")
with k5: st.metric("Median Duration(s)", f"{sf['session_duration'].median():.0f}")
with k6: st.metric("Activation ì„±ê³µë¹„ìœ¨", f"{sf['activation'].mean():.2%}")
st.caption("Activation ê¸°ì¤€: PVâ‰¥3 Â· ì§€ì†ì‹œê°„â‰¥180ì´ˆ Â· Hitsâ‰¥10 Â· Bounce=0 Â· 30ì¼ ë‚´ ì¬ë°©ë¬¸â‰¥2íšŒ")

# =========================
# 3) íƒ­ êµ¬ì„±
# =========================
tabR, tabA, tabRef, tabRev, tabAcq = st.tabs(
    ["Retention", "Activation", "Referral", "Revenue", "Acquisition"]
)

# -------------------------
# Tab 1. Retention
# -------------------------
with tabR:
    st.subheader("1) ì‚¬ìš©ì ë‹¨ìœ„ Retention (30ì¼ ì´ë‚´ ì¬ë°©ë¬¸)")
    user_revisit = label_user_revisit_30d(sf)
    st.metric("30ì¼ ì¬ë°©ë¬¸ìœ¨(ì‚¬ìš©ì ê¸°ì¤€)", f"{user_revisit.mean():.2%}")
    st.write("â€¢ ì •ì˜: ê° ì‚¬ìš©ìì˜ ì²« ì„¸ì…˜ ì´í›„ 30ì¼ ì•ˆì— **í•œ ë²ˆì´ë¼ë„** ì¬ë°©ë¬¸í•˜ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼")

    # ì²« ë°©ë¬¸ ì±„ë„/ë””ë°”ì´ìŠ¤ë³„ Retention
    first_session = sf.sort_values(["fullVisitorId","session_start"]) \
                      .groupby("fullVisitorId", as_index=False) \
                      .head(1)[["fullVisitorId","channel","device","session_start"]]
    first_map_ch  = dict(zip(first_session["fullVisitorId"], first_session["channel"]))
    first_map_dev = dict(zip(first_session["fullVisitorId"], first_session["device"]))
    user_df = user_revisit.reset_index()
    user_df["first_channel"] = user_df["fullVisitorId"].map(first_map_ch)
    user_df["first_device"]  = user_df["fullVisitorId"].map(first_map_dev)

    c1,c2 = st.columns(2)
    with c1:
        ch_tbl = user_df.groupby("first_channel")["revisit_30d"].mean().sort_values(ascending=False).reset_index()
        st.write("2) **ì²« ë°©ë¬¸ ì±„ë„ë³„** 30ì¼ ì¬ë°©ë¬¸ìœ¨"); st.dataframe(ch_tbl)
        st.bar_chart(ch_tbl.set_index("first_channel")["revisit_30d"])
    with c2:
        dev_tbl = user_df.groupby("first_device")["revisit_30d"].mean().sort_values(ascending=False).reset_index()
        st.write("3) **ì²« ë°©ë¬¸ ë””ë°”ì´ìŠ¤ë³„** 30ì¼ ì¬ë°©ë¬¸ìœ¨"); st.dataframe(dev_tbl)
        st.bar_chart(dev_tbl.set_index("first_device")["revisit_30d"])

    st.subheader("5) ì±„ë„ Ã— PVêµ¬ê°„ë³„ êµ¬ë§¤ ì „í™˜ìœ¨")
    pv_bins = [0, 1, 3, 5, 10, 20, 50, 100, np.inf]
    pv_cut = pd.cut(sf["pv"], bins=pv_bins, right=False)
    pv_order = pv_cut.cat.categories.astype(str).tolist()
    sf["pv_bin"] = pv_cut

    conv_flag = sf["is_transaction"] if "is_transaction" in sf.columns else (sf["revenue"] > 0).astype(int)
    sf["_conv_flag"] = conv_flag.astype(int)

    conv_tbl = (sf.groupby(["channel", "pv_bin"], observed=False)
                  .agg(sessions=("session_id", "count"),
                       conversions=("_conv_flag", "sum"))
                  .reset_index())
    conv_tbl["conv_rate"] = np.where(conv_tbl["sessions"] > 0,
                                     conv_tbl["conversions"] / conv_tbl["sessions"], np.nan)
    conv_tbl["pv_bin"] = conv_tbl["pv_bin"].astype(str)

    heat = alt.Chart(conv_tbl).mark_rect().encode(
        x=alt.X("pv_bin:N", sort=pv_order, title="PV êµ¬ê°„"),
        y=alt.Y("channel:N", sort="-x", title="ì±„ë„"),
        color=alt.Color("conv_rate:Q", title="ì „í™˜ìœ¨"),
        tooltip=[alt.Tooltip("channel:N", title="ì±„ë„"),
                 alt.Tooltip("pv_bin:N", title="PV êµ¬ê°„"),
                 alt.Tooltip("sessions:Q", title="ì„¸ì…˜ìˆ˜", format=",.0f"),
                 alt.Tooltip("conv_rate:Q", title="ì „í™˜ìœ¨", format=".2%")]
    ).properties(height=320)
    st.altair_chart(heat, use_container_width=True)
    sf.drop(columns=["_conv_flag"], inplace=True, errors="ignore")

# -------------------------
# Tab 2. Activation
# -------------------------
with tabA:
    st.markdown("""
    **âœ… Activation ì„±ê³µ ê¸°ì¤€**  
    1) PV â‰¥ 3 Â· 2) ì§€ì†ì‹œê°„ â‰¥ 180ì´ˆ Â· 3) Bounce=0 Â· 4) Hits â‰¥ 10 Â· 5) 30ì¼ ë‚´ ì¬ë°©ë¬¸ â‰¥ 2íšŒ
    """)
    st.subheader("1) ì„¸ì…˜ë‹¨ìœ„ Activation ì„±ê³µ ë¹„ìœ¨ (ì¼ë³„)")
    daily = (sf.groupby("session_date", as_index=False)
               .agg(activation=("activation","mean"),
                    sessions=("session_id","count")))
    daily["activation_ma7"] = moving_avg(daily["activation"], 7)
    daily["low_sample"] = daily["sessions"] < min_day_sessions

    base = alt.Chart(daily).encode(x="session_date:T")
    bars = base.mark_bar(opacity=0.3).encode(y=alt.Y("sessions:Q", axis=alt.Axis(title="Sessions")))
    line = base.mark_line().encode(y=alt.Y("activation:Q", axis=alt.Axis(title="Activation Rate", format="~%")))
    ma7  = base.mark_line(strokeDash=[4,4]).encode(y="activation_ma7:Q", color=alt.value("gray"))
    pts  = base.mark_circle(size=20).encode(y="activation:Q",
            color=alt.condition("datum.low_sample", alt.value("#aaaaaa"), alt.value("#1f77b4")))
    st.altair_chart(alt.layer(bars, line, ma7, pts).resolve_scale(y='independent')
                    .properties(height=320, width="container"), use_container_width=True)
    st.caption(f"ì  ìƒ‰ìƒ: ì¼ ì„¸ì…˜ìˆ˜ < {min_day_sessions} (íšŒìƒ‰)")

    st.subheader("2) ì±„ë„/ë””ë°”ì´ìŠ¤ë³„ Activation")
    ch_act = (sf.groupby("channel", observed=False)
                .agg(sessions=("session_id","count"), act_rate=("activation","mean"))
                .sort_values("act_rate", ascending=False)).reset_index()
    st.dataframe(ch_act); st.bar_chart(ch_act.set_index("channel")["act_rate"])

    dev_act = (sf.groupby("device", observed=False)
                 .agg(sessions=("session_id","count"), act_rate=("activation","mean"))
                 .sort_values("act_rate", ascending=False)).reset_index()
    st.dataframe(dev_act); st.bar_chart(dev_act.set_index("device")["act_rate"])

    st.subheader("3) ì‹¬ì¸µ ì§€í‘œ & í¼ë„")
    pv_bins2 = [0,1,3,5,10,20,50,100,np.inf]; pv_cut = pd.cut(sf["pv"], pv_bins2, right=False)
    dur_bins = [0,30,60,120,180,300,600,1200,1800,3600,np.inf]; dur_cut = pd.cut(sf["session_duration"], dur_bins, right=False)
    hit_bins = [0,5,10,20,50,100,200,np.inf]; hit_cut = pd.cut(sf["hits"], hit_bins, right=False)
    pv_act = sf.groupby(pv_cut, observed=False)["activation"].mean().reset_index()
    pv_act["pv_bin"] = pv_act.iloc[:,0].astype(str); pv_act["rate"] = pv_act["activation"]
    st.bar_chart(pv_act.set_index("pv_bin")["rate"])
    funnel = (sf.groupby(pv_cut, observed=False)
                .agg(sessions=("session_id","count"), act_sessions=("activation","sum"))
                .reset_index())
    funnel = funnel.rename(columns={funnel.columns[0]:"pv_bin"})
    funnel["pv_bin"] = funnel["pv_bin"].astype(str)
    funnel["activation_rate"] = np.where(funnel["sessions"]>0, funnel["act_sessions"]/funnel["sessions"], np.nan)
    st.dataframe(funnel)

# -------------------------
# Tab 3. Referral
# -------------------------
with tabRef:
    st.subheader("Referral ë¶„ì„")
    ref = sf[(sf["medium"].str.lower() == "referral") | (sf["channel"] == "Referral")].copy()
    if ref.empty:
        st.warning("Referral ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°” í•„í„°ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    top_sessions = (ref.groupby("source", observed=False)["session_id"]
                      .count().reset_index(name="sessions")
                      .sort_values("sessions", ascending=False).head(10))
    st.markdown("### Top 10 Referral Sources (ì„¸ì…˜ ìˆ˜)")
    ch_sessions = (alt.Chart(top_sessions).mark_bar().encode(
        x=alt.X("sessions:Q", title="Number of Sessions"),
        y=alt.Y("source:N", sort="-x", title="Source"),
        tooltip=["source","sessions"]
    ).properties(height=380))
    st.altair_chart(ch_sessions, use_container_width=True)

    ref_top = ref.merge(top_sessions, on="source", how="inner")
    by_source = (ref_top.groupby("source", observed=False)
                  .agg(avg_pv=("pv","mean"), avg_hits=("hits","mean"),
                       bounce_rate=("bounces","mean"), sessions=("session_id","count"))
                  .reset_index().sort_values("sessions", ascending=False).head(10))
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("#### Avg Pageviews (Top 10)")
        st.altair_chart(alt.Chart(by_source).mark_bar().encode(
            x=alt.X("avg_pv:Q", title="Average Pageviews"),
            y=alt.Y("source:N", sort="-x", title="Source"),
            tooltip=["source", alt.Tooltip("avg_pv:Q", format=".2f")]
        ).properties(height=420), use_container_width=True)
    with c2:
        st.markdown("#### Avg Hits (Top 10)")
        st.altair_chart(alt.Chart(by_source).mark_bar().encode(
            x=alt.X("avg_hits:Q", title="Average Hits"),
            y=alt.Y("source:N", sort="-x", title="Source"),
            tooltip=["source", alt.Tooltip("avg_hits:Q", format=".2f")]
        ).properties(height=420), use_container_width=True)
    with c3:
        st.markdown("#### Bounce Rate (Top 10)")
        st.altair_chart(alt.Chart(by_source.assign(br_pct=by_source["bounce_rate"]*100))
            .mark_bar().encode(
            x=alt.X("br_pct:Q", title="Bounce Rate (%)"),
            y=alt.Y("source:N", sort="-x", title="Source"),
            tooltip=["source", alt.Tooltip("br_pct:Q", format=".1f")]
        ).properties(height=420), use_container_width=True)
    st.markdown("#### Raw Table")
    st.dataframe(by_source[["source","sessions","avg_pv","avg_hits","bounce_rate"]]
                 .rename(columns={"avg_pv":"avg_pageviews","bounce_rate":"bounce_rate(0-1)"}))

# -------------------------
# Tab 4. Revenue
# -------------------------
with tabRev:
    st.subheader("ğŸŒ Distribution of Continent")
    df_f = df[df["session_id"].isin(sf["session_id"])].copy()
    if "geoNetwork_continent" in df_f.columns:
        cont_tbl = (df_f["geoNetwork_continent"].fillna("Unknown")
                    .value_counts().rename_axis("continent").reset_index(name="sessions"))
        chart_cont = alt.Chart(cont_tbl).mark_bar().encode(
            x=alt.X("sessions:Q", title="Sessions"),
            y=alt.Y("continent:N", sort="-x", title=None),
            tooltip=[alt.Tooltip("continent:N", title="Continent"),
                     alt.Tooltip("sessions:Q", title="Sessions", format=",.0f")]
        ).properties(width=420, height=260)
        st.altair_chart(chart_cont, use_container_width=False)
    else:
        st.info("continent ì»¬ëŸ¼ì´ ì—†ì–´ ë¶„í¬ ì°¨íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    st.subheader("ğŸ“± Mobile vs. Desktop Traffic Share")
    dev_cnt = sf["device"].value_counts()
    obs_mobile = int(dev_cnt.get("mobile", 0)); obs_desktop = int(dev_cnt.get("desktop", 0))
    obs_total_md = max(obs_mobile + obs_desktop, 1)
    obs_mobile_share = obs_mobile / obs_total_md * 100.0
    obs_desktop_share = obs_desktop / obs_total_md * 100.0
    market_mobile_share, market_desktop_share = 64.3, 35.7

    fig, ax = plt.subplots(figsize=(5.2, 3.6))
    x = [0, 1]; labels = ["2016â€“2017 (Observed)", "2025 (Market)"]
    ax.bar(x[0], obs_mobile_share, label="Mobile"); ax.bar(x[0], obs_desktop_share, bottom=obs_mobile_share, label="Desktop")
    ax.bar(x[1], market_mobile_share); ax.bar(x[1], market_desktop_share, bottom=market_mobile_share)
    ax.set_xticks(x); ax.set_xticklabels(labels); ax.set_ylim(0, 105); ax.set_ylabel("Traffic Share (%)")
    for i,(m,d) in enumerate([(obs_mobile_share,obs_desktop_share),(market_mobile_share,market_desktop_share)]):
        ax.text(x[i], m/2, f"{m:.1f}%", ha="center", va="center", fontsize=10, weight="bold")
        ax.text(x[i], m+d/2, f"{d:.1f}%", ha="center", va="center", fontsize=10, weight="bold")
    ax.legend(loc="upper center", ncol=2, bbox_to_anchor=(0.5, -0.20), frameon=False)
    st.pyplot(fig, use_container_width=False)

    st.markdown("""
    **ë°œê²¬:** ë°ì´í„°ì…‹ì— ê¸°ë¡ëœ ê±°ë˜ ìˆ˜ìµì´ ë¶€ì¡±/ì—†ì–´ ì§ì ‘ ìˆ˜ìµ ì˜ˆì¸¡ì€ ì–´ë µìŠµë‹ˆë‹¤.  
    **ì‹œì‚¬ì :** ì´íƒˆë¥  ê°ì†Œì™€ ì°¸ì—¬ë„ ì¦ëŒ€(í˜ì´ì§€ë·°/ì²´ë¥˜ì‹œê°„â†‘)ê°€ ì „í™˜/ìˆ˜ìµ ì¦ëŒ€ì˜ ì„ í–‰ ê³¼ì œì…ë‹ˆë‹¤.
    """)

# -------------------------
# Tab 5. Acquisition (ê°„ë‹¨ ë²„ì „: ì‹œê°í™” ìœ„ì£¼)
# -------------------------
with tabAcq:
    plt.rc('font', family='Malgun Gothic'); plt.rc('axes', unicode_minus=False)
    st.header("1ï¸âƒ£ ìœ ì… ê·œëª¨ & ì‹ ê·œ ê³ ê°")

    # ì±„ë„ë³„ ì„¸ì…˜ & ì‹ ê·œ
    if {"channelGrouping","totals_newVisits"}.issubset(df.columns):
        channel_summary = (df.groupby("channelGrouping")
                             .agg(sessions=("session_id","nunique"),
                                  new_sessions=("totals_newVisits","sum")).reset_index())
        channel_summary["new_visit_ratio"] = channel_summary["new_sessions"] / channel_summary["sessions"]
        fig, ax1 = plt.subplots(figsize=(8,5))
        ax1.bar(channel_summary["channelGrouping"], channel_summary["sessions"], alpha=0.7)
        ax1.set_ylabel("Sessions", color="blue")
        ax2 = ax1.twinx()
        ax2.plot(channel_summary["channelGrouping"], channel_summary["new_visit_ratio"], color="red", marker="o")
        ax2.set_ylabel("New Visit Ratio", color="red")
        plt.xticks(rotation=45); st.pyplot(fig)
    else:
        st.info("ì±„ë„/ì‹ ê·œ ë°©ë¬¸ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    # Device Ã— Channel íˆíŠ¸ë§µ
    if {"device_deviceCategory","channelGrouping","totals_pageviews"}.issubset(df.columns):
        pivot = df.pivot_table(index="device_deviceCategory", columns="channelGrouping",
                               values="totals_pageviews", aggfunc="mean")
        fig, ax = plt.subplots(figsize=(10,5))
        sns.heatmap(pivot, annot=True, fmt=".1f", cmap="YlGnBu", ax=ax)
        plt.title("Average Pageviews by Device Ã— Channel"); st.pyplot(fig)

    st.header("4ï¸âƒ£ ì±„ë„ë³„ ì‹ ê·œ vs ì¬ë°©ë¬¸ ë¹„ìœ¨")
    if {"channelGrouping","totals_newVisits"}.issubset(df.columns):
        channel_visits = (df.groupby("channelGrouping")["totals_newVisits"]
                            .agg(sessions="count", new_sessions="sum").reset_index())
        channel_visits["repeat_sessions"] = channel_visits["sessions"] - channel_visits["new_sessions"]
        channel_visits["new_ratio"] = channel_visits["new_sessions"] / channel_visits["sessions"]
        channel_visits["repeat_ratio"] = channel_visits["repeat_sessions"] / channel_visits["sessions"]
        fig, ax = plt.subplots(figsize=(10,6))
        ax.bar(channel_visits["channelGrouping"], channel_visits["new_ratio"], label="ì‹ ê·œ")
        ax.bar(channel_visits["channelGrouping"], channel_visits["repeat_ratio"],
               bottom=channel_visits["new_ratio"], label="ì¬ë°©ë¬¸")
        ax.set_ylabel("ë¹„ìœ¨"); ax.set_xlabel("ì±„ë„"); plt.xticks(rotation=45); plt.legend()
        plt.title("ì±„ë„ë³„ ì‹ ê·œ vs ì¬ë°©ë¬¸ ë¹„ìœ¨"); st.pyplot(fig)

st.success("ì™„ë£Œ! ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ê¸°ê°„/ì±„ë„/ë””ë°”ì´ìŠ¤ë¥¼ ë°”ê¿”ê°€ë©° Retention~Acquisitionì„ íƒìƒ‰í•˜ì„¸ìš”.")
