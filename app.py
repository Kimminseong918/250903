# ì‹¤í–‰: streamlit run app.py

import io
import pandas as pd
import numpy as np
import streamlit as st
import altair as alt
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
import matplotlib
from datetime import timedelta

# =========================
# 0) ê¸°ë³¸ ì„¤ì •
# =========================
SESSION_GAP_MIN = 30  # (ë°ì´í„°ì— session_idê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)

st.set_page_config(page_title="RARRA Dashboard", layout="wide")
st.title("ğŸ“Š RARRA Dashboard (Retention â€¢ Activation â€¢ Referral â€¢ Revenue â€¢ Acquisition)")
st.caption("Kaggle GA Customer Revenue Dataset | ì„¸ì…˜ ë‹¨ìœ„ ë¶„ì„")

# ì‹œìŠ¤í…œ í°íŠ¸(ë§êµ½/í•œê¸€) ë³´ì •: ë°°í¬ í™˜ê²½ì— ì—†ìœ¼ë©´ ìë™ ë¬´ì‹œ
if "Malgun Gothic" in [f.name for f in matplotlib.font_manager.fontManager.ttflist]:
    plt.rc("font", family="Malgun Gothic")
    plt.rc("axes", unicode_minus=False)

# =========================
# 1) ë°ì´í„° ë¡œë”© & ì „ì²˜ë¦¬
# =========================
@st.cache_data(show_spinner=True)
def load_df_from_file(file) -> pd.DataFrame:
    """file_uploaderê°€ ë°˜í™˜í•œ file-like/bytesì—ì„œ CSV ë¡œë“œ"""
    try:
        df = pd.read_csv(file, dtype=str, low_memory=False)
    except Exception:
        # BytesIOë¡œ ì¬ì‹œë„(utf-8-sig)
        buf = io.BytesIO(file.getvalue() if hasattr(file, "getvalue") else file)
        df = pd.read_csv(buf, encoding="utf-8-sig", dtype=str, low_memory=False)

    # í•„ìˆ˜ ì»¬ëŸ¼
    if "event_time" not in df.columns:
        raise ValueError("event_time ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    # ì‹œê°„ ì²˜ë¦¬
    df["event_time"] = pd.to_datetime(df["event_time"], utc=True, errors="coerce")
    df["event_time_naive"] = df["event_time"].dt.tz_convert("UTC").dt.tz_localize(None)

    # ìˆ«ìí˜• ë³€í™˜ ìœ í‹¸
    def to_num_frame(_df, cols):
        for col in cols:
            if col in _df.columns:
                _df[col] = pd.to_numeric(_df[col], errors="coerce").fillna(0)
        return _df

    df = to_num_frame(
        df,
        [
            "totals_pageviews",
            "totals_hits",
            "totals_bounces",
            "totals_newVisits",
            "totals_transactionRevenue",
            "session_duration",
            "is_transaction",
            "visitNumber",
        ],
    )

    if "fullVisitorId" in df.columns:
        df["fullVisitorId"] = df["fullVisitorId"].astype(str)

    # session_idê°€ ì—†ìœ¼ë©´ 30ë¶„ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
    if "session_id" not in df.columns:
        df = df.sort_values(["fullVisitorId", "event_time_naive"]).reset_index(drop=True)
        diff = df.groupby("fullVisitorId")["event_time_naive"].diff().dt.total_seconds()
        df["new_session"] = (diff.isna()) | (diff > SESSION_GAP_MIN * 60)
        df["session_num"] = df.groupby("fullVisitorId")["new_session"].cumsum().astype(int)
        df["session_id"] = df["fullVisitorId"] + "_" + df["session_num"].astype(str)

    # ê²°ì¸¡ ë³´ì •(ì—†ìœ¼ë©´ Unknown)
    for c in [
        "channelGrouping",
        "device_deviceCategory",
        "trafficSource_source",
        "trafficSource_medium",
        "trafficSource_referralPath",
    ]:
        if c not in df.columns:
            df[c] = "Unknown"

    return df


@st.cache_data(show_spinner=False)
def build_session_table(df: pd.DataFrame) -> pd.DataFrame:
    sess = (
        df.groupby("session_id", as_index=False)
        .agg(
            fullVisitorId=("fullVisitorId", "first"),
            session_start=("event_time_naive", "min"),
            session_end=("event_time_naive", "max"),
            pv=("totals_pageviews", "sum"),
            hits=("totals_hits", "sum"),
            bounces=("totals_bounces", "max"),
            newVisit=("totals_newVisits", "max"),
            visitNumber=("visitNumber", "max"),
            revenue=("totals_transactionRevenue", "sum"),
            channel=("channelGrouping", "first"),
            device=("device_deviceCategory", "first"),
            session_duration=("session_duration", "max"),
            source=("trafficSource_source", "first"),
            medium=("trafficSource_medium", "first"),
            referral_path=("trafficSource_referralPath", "first"),
        )
        .copy()
    )

    for c in ["source", "medium", "referral_path"]:
        sess[c] = sess[c].fillna("Unknown")

    # íŒŒìƒ
    sess["session_date"] = pd.to_datetime(sess["session_start"]).dt.date
    sess["session_hour"] = pd.to_datetime(sess["session_start"]).dt.hour
    sess["first_week"] = pd.to_datetime(sess["session_start"]).dt.to_period("W")
    sess["is_transaction"] = (sess["revenue"] > 0).astype(int)

    # 30ì¼ ë‚´ ì¬ë°©ë¬¸ ëˆ„ì  ì¹´ìš´íŠ¸(ì„¸ì…˜ë³„)
    def compute_revisit_count_30d(_sess: pd.DataFrame) -> pd.DataFrame:
        s = _sess.sort_values(["fullVisitorId", "session_start"]).copy()
        first = s.groupby("fullVisitorId")["session_start"].transform("min")
        s["within_30d"] = (s["session_start"] > first) & (s["session_start"] <= first + pd.Timedelta(days=30))
        s["revisit_count_30d"] = s.groupby("fullVisitorId")["within_30d"].cumsum()
        return s[["session_id", "revisit_count_30d"]]

    revisit = compute_revisit_count_30d(sess)
    sess = sess.merge(revisit, on="session_id", how="left")
    sess["revisit_count_30d"] = sess["revisit_count_30d"].fillna(0).astype(int)

    # ì²« ë°©ë¬¸ ì±„ë„/ë””ë°”ì´ìŠ¤ ë¼ë²¨
    first_idx = (
        sess.sort_values(["fullVisitorId", "session_start"]).groupby("fullVisitorId", as_index=False).head(1)
    )
    sess["first_channel"] = sess["fullVisitorId"].map(dict(zip(first_idx["fullVisitorId"], first_idx["channel"])))
    sess["first_device"] = sess["fullVisitorId"].map(dict(zip(first_idx["fullVisitorId"], first_idx["device"])))

    for c in ["pv", "hits", "bounces", "visitNumber", "session_duration"]:
        sess[c] = sess[c].fillna(0)

    return sess


def label_user_revisit_30d(sess: pd.DataFrame) -> pd.DataFrame:
    s = sess.copy()
    first = s.groupby("fullVisitorId")["session_start"].transform("min")
    s["within_30d"] = (s["session_start"] > first) & (s["session_start"] <= first + pd.Timedelta(days=30))
    return s.groupby("fullVisitorId")["within_30d"].any().rename("revisit_30d")


def moving_avg(series: pd.Series, k: int = 7) -> pd.Series:
    return series.rolling(k, min_periods=1).mean()

# =========================
# 2) ì‚¬ì´ë“œë°”: ì—…ë¡œë“œ & í•„í„°
# =========================
with st.sidebar:
    st.header("ë°ì´í„° ì—…ë¡œë“œ")
    uploaded = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["csv"])
    st.caption("UTF-8/utf-8-sig ê¶Œì¥. 100MB ì´í•˜ ê¶Œì¥.")

if uploaded is None:
    st.info("ì™¼ìª½ì—ì„œ CSVë¥¼ ì—…ë¡œë“œí•˜ë©´ ëŒ€ì‹œë³´ë“œê°€ ìƒì„±ë©ë‹ˆë‹¤.")
    st.stop()

df = load_df_from_file(uploaded)
sess = build_session_table(df)

# ë‚ ì§œ/ì±„ë„/ë””ë°”ì´ìŠ¤ í•„í„°
min_d, max_d = sess["session_date"].min(), sess["session_date"].max()
with st.sidebar:
    st.markdown("---")
    st.header("í•„í„°")
    start_d = st.date_input("ì‹œì‘ì¼", min_d, min_value=min_d, max_value=max_d)
    end_d = st.date_input("ì¢…ë£Œì¼", max_d, min_value=min_d, max_value=max_d)

    channels = sorted(sess["channel"].dropna().unique().tolist())
    devices = sorted(sess["device"].dropna().unique().tolist())
    sel_channels = st.multiselect("ì±„ë„", channels, default=channels)
    sel_devices = st.multiselect("ë””ë°”ì´ìŠ¤", devices, default=devices)

    st.subheader("Activation ê¸°ì¤€")
    min_day_sessions = st.number_input("ì¼ë³„ í‘œë³¸ ìµœì†Œ ì„¸ì…˜ìˆ˜(ìŠ¤íŒŒì´í¬ ê°€ë“œë ˆì¼)", 0, value=500, step=50)

# í•„í„° ì ìš©
mask = (
    (sess["session_date"] >= start_d)
    & (sess["session_date"] <= end_d)
    & (sess["channel"].isin(sel_channels))
    & (sess["device"].isin(sel_devices))
)
sf = sess.loc[mask].copy()

# =========================
# 3) Activation í”Œë˜ê·¸ ê³„ì‚° (ì„¸ì…˜ ë‹¨ìœ„)
# =========================
if "revisit_count_30d" not in sf.columns:
    sf = sf.merge(sess[["session_id", "revisit_count_30d"]], on="session_id", how="left")
sf["revisit_count_30d"] = sf["revisit_count_30d"].fillna(0).astype(int)

sf["act_pageviews"] = (sf["pv"] >= 3).astype(int)
sf["act_duration"] = (sf["session_duration"] >= 180).astype(int)  # 3ë¶„
sf["act_nonbounce"] = (sf["bounces"] == 0).astype(int)
sf["act_hits"] = (sf["hits"] >= 10).astype(int)
sf["act_revisit"] = (sf["revisit_count_30d"] >= 2).astype(int)

sf["activation"] = (
    (sf["act_pageviews"] == 1)
    & (sf["act_duration"] == 1)
    & (sf["act_nonbounce"] == 1)
    & (sf["act_hits"] == 1)
    & (sf["act_revisit"] == 1)
).astype(int)

# =========================
# ìƒë‹¨ KPI
# =========================
k1, k2, k3, k4, k5, k6 = st.columns(6)
with k1:
    st.metric("ì‚¬ìš©ì ìˆ˜", f"{sf['fullVisitorId'].nunique():,}")
with k2:
    st.metric("ì„¸ì…˜ ìˆ˜", f"{sf['session_id'].nunique():,}")
with k3:
    st.metric("í‰ê·  PV/ì„¸ì…˜", f"{sf['pv'].mean():.2f}")
with k4:
    st.metric("Bounce Rate", f"{sf['bounces'].mean():.2%}")
with k5:
    st.metric("Median Duration(s)", f"{sf['session_duration'].median():.0f}")
with k6:
    st.metric("Activation ì„±ê³µë¹„ìœ¨", f"{sf['activation'].mean():.2%}")

st.caption("Activation ê¸°ì¤€: PVâ‰¥3 Â· ì§€ì†ì‹œê°„â‰¥180ì´ˆ Â· Hitsâ‰¥10 Â· Bounce=0 Â· 30ì¼ ë‚´ ì¬ë°©ë¬¸â‰¥2íšŒ")

# =========================
# 4) íƒ­ êµ¬ì„±
# =========================
tabR, tabA, tabRef, tabRev, tabAcq = st.tabs(
    ["Retention", "Activation", "Referral", "Revenue", "Acquisition"]
)

# -------------------------
# Tab 1. Retention
# -------------------------
with tabR:
    st.subheader("1) ì‚¬ìš©ì ë‹¨ìœ„ Retention (30ì¼ ì´ë‚´ ì¬ë°©ë¬¸)")
    user_revisit = label_user_revisit_30d(sf)
    st.metric("30ì¼ ì¬ë°©ë¬¸ìœ¨(ì‚¬ìš©ì ê¸°ì¤€)", f"{user_revisit.mean():.2%}")
    st.write("â€¢ ì •ì˜: ê° ì‚¬ìš©ìì˜ ì²« ì„¸ì…˜ ì´í›„ 30ì¼ ì•ˆì— **í•œ ë²ˆì´ë¼ë„** ì¬ë°©ë¬¸í•˜ë©´ ì„±ê³µ")

    # ì²« ë°©ë¬¸ ì±„ë„/ë””ë°”ì´ìŠ¤ë³„
    first_session = (
        sf.sort_values(["fullVisitorId", "session_start"])
        .groupby("fullVisitorId", as_index=False)
        .head(1)[["fullVisitorId", "channel", "device", "session_start"]]
    )
    user_df = user_revisit.reset_index()
    user_df["first_channel"] = user_df["fullVisitorId"].map(dict(zip(first_session["fullVisitorId"], first_session["channel"])))
    user_df["first_device"] = user_df["fullVisitorId"].map(dict(zip(first_session["fullVisitorId"], first_session["device"])))

    c1, c2 = st.columns(2)
    with c1:
        st.write("2) **ì²« ë°©ë¬¸ ì±„ë„ë³„** 30ì¼ ì¬ë°©ë¬¸ìœ¨")
        ch_tbl = user_df.groupby("first_channel")["revisit_30d"].mean().sort_values(ascending=False).reset_index()
        st.dataframe(ch_tbl)
        st.bar_chart(ch_tbl.set_index("first_channel")["revisit_30d"])
    with c2:
        st.write("3) **ì²« ë°©ë¬¸ ë””ë°”ì´ìŠ¤ë³„** 30ì¼ ì¬ë°©ë¬¸ìœ¨")
        dev_tbl = user_df.groupby("first_device")["revisit_30d"].mean().sort_values(ascending=False).reset_index()
        st.dataframe(dev_tbl)
        st.bar_chart(dev_tbl.set_index("first_device")["revisit_30d"])

    st.subheader("4) ê³ ê° ì¶©ì„±ë„/ì°¸ì—¬ë„")
    u_eng = (
        sf.groupby("fullVisitorId")
        .agg(
            sessions=("session_id", "nunique"),
            pv_mean=("pv", "mean"),
            dur_mean=("session_duration", "mean"),
            bounce_rate=("bounces", "mean"),
        )
        .copy()
    )
    c3, c4, c5 = st.columns(3)
    with c3:
        st.metric("ì‚¬ìš©ìë‹¹ ì¤‘ìœ„ ì„¸ì…˜ìˆ˜", f"{u_eng['sessions'].median():.1f}")
    with c4:
        st.metric("ì‚¬ìš©ìë‹¹ í‰ê·  PV", f"{u_eng['pv_mean'].mean():.2f}")
    with c5:
        st.metric("ì‚¬ìš©ìë‹¹ í‰ê·  Bounce Rate", f"{u_eng['bounce_rate'].mean():.2%}")
    st.dataframe(u_eng.head(20))

    st.subheader("5) ì±„ë„ Ã— PVêµ¬ê°„ë³„ êµ¬ë§¤ ì „í™˜ìœ¨")
    pv_bins = [0, 1, 3, 5, 10, 20, 50, 100, np.inf]
    pv_cut = pd.cut(sf["pv"], bins=pv_bins, right=False)
    pv_order = pv_cut.cat.categories.astype(str).tolist()
    sf["pv_bin"] = pv_cut

    conv_flag = sf["is_transaction"].astype(int) if "is_transaction" in sf.columns else (sf["revenue"] > 0).astype(int)
    sf["_conv_flag"] = conv_flag

    conv_tbl = (
        sf.groupby(["channel", "pv_bin"], observed=False)
        .agg(sessions=("session_id", "count"), conversions=("_conv_flag", "sum"))
        .reset_index()
    )
    conv_tbl["conv_rate"] = np.where(
        conv_tbl["sessions"] > 0, conv_tbl["conversions"] / conv_tbl["sessions"], np.nan
    )
    conv_tbl["pv_bin"] = conv_tbl["pv_bin"].astype(str)

    heat = (
        alt.Chart(conv_tbl)
        .mark_rect()
        .encode(
            x=alt.X("pv_bin:N", sort=pv_order, title="PV êµ¬ê°„"),
            y=alt.Y("channel:N", sort="-x", title="ì±„ë„"),
            color=alt.Color("conv_rate:Q", title="ì „í™˜ìœ¨"),
            tooltip=[
                alt.Tooltip("channel:N", title="ì±„ë„"),
                alt.Tooltip("pv_bin:N", title="PV êµ¬ê°„"),
                alt.Tooltip("sessions:Q", title="ì„¸ì…˜ìˆ˜", format=",.0f"),
                alt.Tooltip("conv_rate:Q", title="ì „í™˜ìœ¨", format=".2%"),
            ],
        )
        .properties(height=320)
    )
    st.altair_chart(heat, use_container_width=True)
    sf.drop(columns=["_conv_flag"], inplace=True, errors="ignore")

# -------------------------
# Tab 2. Activation
# -------------------------
with tabA:
    st.markdown(
        """
    **âœ… Activation ì„±ê³µ ê¸°ì¤€**  
    1. í˜ì´ì§€ë·°(PV) â‰¥ 3 Â· 2. ì„¸ì…˜ ì§€ì†ì‹œê°„ â‰¥ 180ì´ˆ Â· 3. Bounce=0  
    4. Hits â‰¥ 10 Â· 5. 30ì¼ ë‚´ ì¬ë°©ë¬¸ íšŸìˆ˜ â‰¥ 2íšŒ
    """
    )
    st.subheader("1) ì„¸ì…˜ë‹¨ìœ„ Activation ì„±ê³µ ë¹„ìœ¨ (ì¼ë³„)")

    daily = sf.groupby("session_date", as_index=False).agg(activation=("activation", "mean"), sessions=("session_id", "count"))
    daily["activation_ma7"] = moving_avg(daily["activation"], 7)
    daily["low_sample"] = daily["sessions"] < min_day_sessions

    base = alt.Chart(daily).encode(x="session_date:T")
    bars = base.mark_bar(opacity=0.3).encode(y=alt.Y("sessions:Q", axis=alt.Axis(title="Sessions")))
    line = base.mark_line().encode(y=alt.Y("activation:Q", axis=alt.Axis(title="Activation Rate", format="~%")))
    ma7 = base.mark_line(strokeDash=[4, 4]).encode(y="activation_ma7:Q", color=alt.value("gray"))
    pts = base.mark_circle(size=20).encode(
        y="activation:Q",
        color=alt.condition("datum.low_sample", alt.value("#aaaaaa"), alt.value("#1f77b4")),
    )
    st.altair_chart(
        alt.layer(bars, line, ma7, pts).resolve_scale(y="independent").properties(height=320, width="container"),
        use_container_width=True,
    )
    st.caption(f"ì  ìƒ‰ìƒ: ì¼ ì„¸ì…˜ìˆ˜ < {min_day_sessions} (íšŒìƒ‰)")

    st.subheader("2) ì±„ë„ë³„ Activation ì„±ê³µ ë¹„ìœ¨")
    ch_act = (
        sf.groupby("channel", observed=False)
        .agg(sessions=("session_id", "count"), act_rate=("activation", "mean"))
        .sort_values("act_rate", ascending=False)
        .reset_index()
    )
    st.dataframe(ch_act)
    st.bar_chart(ch_act.set_index("channel")["act_rate"])

    st.subheader("3) ë””ë°”ì´ìŠ¤ë³„ Activation ì„±ê³µ ë¹„ìœ¨")
    dev_act = (
        sf.groupby("device", observed=False)
        .agg(sessions=("session_id", "count"), act_rate=("activation", "mean"))
        .sort_values("act_rate", ascending=False)
        .reset_index()
    )
    st.dataframe(dev_act)
    st.bar_chart(dev_act.set_index("device")["act_rate"])

    st.subheader("4) ì£¼ì°¨ë³„ ì‹ ê·œ ìœ ì € Activation ë‹¬ì„±ë¥ ")
    first_week_user = sf.sort_values(["fullVisitorId", "session_start"]).groupby("fullVisitorId", as_index=False).head(1)
    fw_map = dict(
        zip(first_week_user["fullVisitorId"], pd.to_datetime(first_week_user["session_start"]).dt.to_period("W"))
    )
    sf["first_week"] = sf["fullVisitorId"].map(fw_map)
    cohort_rate = sf.groupby("first_week", observed=False)["activation"].mean().reset_index()
    cohort_rate["first_week"] = cohort_rate["first_week"].astype(str)
    st.line_chart(cohort_rate.set_index("first_week"))

    st.subheader("5) Activation ì‹¬ì¸µ ì§€í‘œ")
    def pct(v): return alt.Tooltip(v, format=".2%")
    def num(v): return alt.Tooltip(v, format=",.0f")

    pv_bins2 = [0, 1, 3, 5, 10, 20, 50, 100, np.inf]
    pv_cut2 = pd.cut(sf["pv"], pv_bins2, right=False)
    pv_order2 = pv_cut2.cat.categories.astype(str).tolist()
    pv_tbl = (
        sf.groupby(pv_cut2, observed=False)
        .agg(sessions=("session_id", "count"), act_rate=("activation", "mean"))
        .reset_index()
    )
    pv_tbl["pv_bin"] = pv_tbl.iloc[:, 0].astype(str)

    dur_bins = [0, 30, 60, 120, 180, 300, 600, 1200, 1800, 3600, np.inf]
    dur_cut = pd.cut(sf["session_duration"], dur_bins, right=False)
    dur_order = dur_cut.cat.categories.astype(str).tolist()
    dur_tbl = (
        sf.groupby(dur_cut, observed=False)
        .agg(sessions=("session_id", "count"), act_rate=("activation", "mean"))
        .reset_index()
    )
    dur_tbl["dur_bin"] = dur_tbl.iloc[:, 0].astype(str)

    hit_bins = [0, 5, 10, 20, 50, 100, 200, np.inf]
    hit_cut = pd.cut(sf["hits"], hit_bins, right=False)
    hit_order = hit_cut.cat.categories.astype(str).tolist()
    hit_tbl = (
        sf.groupby(hit_cut, observed=False)
        .agg(sessions=("session_id", "count"), act_rate=("activation", "mean"))
        .reset_index()
    )
    hit_tbl["hit_bin"] = hit_tbl.iloc[:, 0].astype(str)

    y_max = float(pd.concat([pv_tbl["act_rate"], dur_tbl["act_rate"], hit_tbl["act_rate"]]).fillna(0).max()) * 1.05 or 0.05

    chart_pv = (
        alt.Chart(pv_tbl, title="PV êµ¬ê°„ë³„ Activation")
        .mark_bar()
        .encode(
            x=alt.X("pv_bin:N", title="PV êµ¬ê°„", sort=pv_order2, axis=alt.Axis(labelAngle=-45)),
            y=alt.Y("act_rate:Q", title="Activation Rate", axis=alt.Axis(format="~%"), scale=alt.Scale(domain=[0, y_max])),
            tooltip=["pv_bin", num("sessions:Q").title("ì„¸ì…˜ ìˆ˜"), pct("act_rate:Q").title("Activation")],
        )
        .properties(height=320)
    )
    chart_dur = (
        alt.Chart(dur_tbl, title="ì§€ì†ì‹œê°„(ì´ˆ) êµ¬ê°„ë³„ Activation")
        .mark_bar()
        .encode(
            x=alt.X("dur_bin:N", title="ì§€ì†ì‹œê°„(ì´ˆ) êµ¬ê°„", sort=dur_order, axis=alt.Axis(labelAngle=-45)),
            y=alt.Y("act_rate:Q", title="Activation Rate", axis=alt.Axis(format="~%"), scale=alt.Scale(domain=[0, y_max])),
            tooltip=["dur_bin", num("sessions:Q").title("ì„¸ì…˜ ìˆ˜"), pct("act_rate:Q").title("Activation")],
        )
        .properties(height=320)
    )
    chart_hit = (
        alt.Chart(hit_tbl, title="Hits êµ¬ê°„ë³„ Activation")
        .mark_bar()
        .encode(
            x=alt.X("hit_bin:N", title="Hits êµ¬ê°„", sort=hit_order, axis=alt.Axis(labelAngle=-45)),
            y=alt.Y("act_rate:Q", title="Activation Rate", axis=alt.Axis(format="~%"), scale=alt.Scale(domain=[0, y_max])),
            tooltip=["hit_bin", num("sessions:Q").title("ì„¸ì…˜ ìˆ˜"), pct("act_rate:Q").title("Activation")],
        )
        .properties(height=320)
    )
    c1, c2, c3 = st.columns(3)
    with c1: st.altair_chart(chart_pv, use_container_width=True)
    with c2: st.altair_chart(chart_dur, use_container_width=True)
    with c3: st.altair_chart(chart_hit, use_container_width=True)

    st.subheader("6) í¼ë„ ê´€ì : PV êµ¬ê°„ë³„ ì„¸ì…˜ ìˆ˜ & Activation")
    funnel = (
        sf.groupby(pv_cut2, observed=False)
        .agg(sessions=("session_id", "count"), act_sessions=("activation", "sum"))
        .reset_index()
        .rename(columns={sf.groupby(pv_cut2, observed=False).agg(sessions=("session_id","count")).reset_index().columns[0]: "pv_bin"})
    )
    funnel["pv_bin"] = funnel["pv_bin"].astype(str)
    funnel["activation_rate"] = np.where(funnel["sessions"] > 0, funnel["act_sessions"] / funnel["sessions"], np.nan)
    st.dataframe(funnel)

# -------------------------
# Tab 3. Referral
# -------------------------
with tabRef:
    st.subheader("Referral ë¶„ì„")
    ref = sf[(sf["medium"].str.lower() == "referral") | (sf["channel"] == "Referral")].copy()
    if ref.empty:
        st.warning("Referral ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°” í•„í„°ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    else:
        top_sessions = (
            ref.groupby("source", observed=False)["session_id"].count().reset_index(name="sessions").sort_values("sessions", ascending=False).head(10)
        )
        st.markdown("### Top 10 Referral Sources (ì„¸ì…˜ ìˆ˜)")
        st.altair_chart(
            alt.Chart(top_sessions)
            .mark_bar()
            .encode(
                x=alt.X("sessions:Q", title="Number of Sessions"),
                y=alt.Y("source:N", sort="-x", title="Source"),
                tooltip=["source", "sessions"],
            )
            .properties(height=380),
            use_container_width=True,
        )

        st.markdown("---")
        ref_top = ref.merge(top_sessions, on="source", how="inner")
        by_source = (
            ref_top.groupby("source", observed=False)
            .agg(avg_pv=("pv", "mean"), avg_hits=("hits", "mean"), bounce_rate=("bounces", "mean"), sessions=("session_id", "count"))
            .reset_index()
            .sort_values("sessions", ascending=False)
            .head(10)
        )

        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown("#### Avg Pageviews by Referral Source (Top 10)")
            st.altair_chart(
                alt.Chart(by_source).mark_bar().encode(
                    x=alt.X("avg_pv:Q", title="Average Pageviews"),
                    y=alt.Y("source:N", sort="-x", title="Source"),
                    tooltip=["source", alt.Tooltip("avg_pv:Q", format=".2f")],
                ).properties(height=420),
                use_container_width=True,
            )
        with col2:
            st.markdown("#### Avg Hits by Referral Source (Top 10)")
            st.altair_chart(
                alt.Chart(by_source).mark_bar().encode(
                    x=alt.X("avg_hits:Q", title="Average Hits"),
                    y=alt.Y("source:N", sort="-x", title="Source"),
                    tooltip=["source", alt.Tooltip("avg_hits:Q", format=".2f")],
                ).properties(height=420),
                use_container_width=True,
            )
        with col3:
            st.markdown("#### Bounce Rate by Referral Source (Top 10)")
            st.altair_chart(
                alt.Chart(by_source.assign(br_pct=by_source["bounce_rate"] * 100))
                .mark_bar()
                .encode(
                    x=alt.X("br_pct:Q", title="Bounce Rate (%)"),
                    y=alt.Y("source:N", sort="-x", title="Source"),
                    tooltip=["source", alt.Tooltip("br_pct:Q", format=".1f")],
                )
                .properties(height=420),
                use_container_width=True,
            )
        st.markdown("#### Raw Table")
        show_cols = ["source", "sessions", "avg_pv", "avg_hits", "bounce_rate"]
        st.dataframe(by_source[show_cols].rename(columns={"avg_pv": "avg_pageviews", "bounce_rate": "bounce_rate(0-1)"}))

# -------------------------
# Tab 4. Revenue
# -------------------------
with tabRev:
    st.subheader("ğŸŒ Distribution of Continent")
    df_f = df[df["session_id"].isin(sf["session_id"])].copy()
    cont_tbl = (
        df_f["geoNetwork_continent"].fillna("Unknown").value_counts().rename_axis("continent").reset_index(name="sessions")
    )
    chart_cont = (
        alt.Chart(cont_tbl)
        .mark_bar()
        .encode(
            x=alt.X("sessions:Q", title="Sessions"),
            y=alt.Y("continent:N", sort="-x", title=None),
            tooltip=[alt.Tooltip("continent:N", title="Continent"), alt.Tooltip("sessions:Q", title="Sessions", format=",.0f")],
        )
        .properties(width=420, height=260)
    )
    st.altair_chart(chart_cont, use_container_width=False)

    st.subheader("ğŸ“± Mobile vs. Desktop Traffic Share")
    dev_cnt = sf["device"].value_counts()
    obs_mobile = int(dev_cnt.get("mobile", 0))
    obs_desktop = int(dev_cnt.get("desktop", 0))
    obs_total_md = max(obs_mobile + obs_desktop, 1)
    obs_mobile_share = obs_mobile / obs_total_md * 100.0
    obs_desktop_share = obs_desktop / obs_total_md * 100.0

    market_mobile_share = 64.3
    market_desktop_share = 35.7

    fig, ax = plt.subplots(figsize=(5.2, 3.6))
    x = [0, 1]
    labels = ["2016â€“2017 (Observed)", "2025 (Market)"]
    ax.bar(x[0], obs_mobile_share, label="Mobile")
    ax.bar(x[0], obs_desktop_share, bottom=obs_mobile_share, label="Desktop")
    ax.bar(x[1], market_mobile_share)
    ax.bar(x[1], market_desktop_share, bottom=market_mobile_share)
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=0)
    ax.set_ylim(0, 105)
    ax.set_ylabel("Traffic Share (%)")
    ax.text(x[0], obs_mobile_share / 2, f"{obs_mobile_share:.1f}%", ha="center", va="center", fontsize=10, weight="bold")
    ax.text(x[0], obs_mobile_share + obs_desktop_share / 2, f"{obs_desktop_share:.1f}%", ha="center", va="center", fontsize=10, weight="bold")
    ax.text(x[1], market_mobile_share / 2, f"{market_mobile_share:.1f}%", ha="center", va="center", fontsize=10, weight="bold")
    ax.text(x[1], market_mobile_share + market_desktop_share / 2, f"{market_desktop_share:.1f}%", ha="center", va="center", fontsize=10, weight="bold")
    ax.legend(loc="upper center", ncol=2, bbox_to_anchor=(0.5, -0.20), frameon=False)
    st.pyplot(fig, use_container_width=False)

    st.markdown(
        """
**2025ë…„: ëª¨ë°”ì¼ 64.3%, ë°ìŠ¤í¬í†± 35.7% â†’ ëª¨ë°”ì¼ ì¤‘ì‹¬ êµ¬ì¡°**  
â¡ï¸ **ëª¨ë°”ì¼ ë¯¹ìŠ¤ ê²°í•**ì˜ ê²©ì°¨ë¥¼ ì‹œê°í™”. í–¥í›„ **Mobile-First** ì „ëµ í•„ìš”.
        """
    )
    st.markdown(
        """
**ë°œê²¬ ì‚¬í•­**
- ê±°ë˜ ìˆ˜ìµì´ ê±°ì˜ ì—†ì–´ ì§ì ‘ì  ìˆ˜ìµ ë¶„ì„ì€ ì œí•œì .
- ë†’ì€ ì´íƒˆ/ë‚®ì€ ì°¸ì—¬ë„ëŠ” ì ì¬ì  ìˆ˜ìµ ê¸°íšŒ ì†ì‹¤.

**ì „ëµ ì‹œì‚¬ì **
- ì°¸ì—¬ë„ ì¦ì§„(ì´íƒˆë¥ â†“, PVâ†‘)ì´ ì „í™˜/ìˆ˜ìµì˜ ì „ì œ.
- UX/UI ê°œì„ Â·íƒ€ê²ŸíŒ… ìµœì í™” ì‹¤í–‰ì— ì§‘ì¤‘.
        """
    )

# -------------------------
# Tab 5. Acquisition
# -------------------------
with tabAcq:
    st.header("1ï¸âƒ£ ìœ ì… ê·œëª¨ & ì‹ ê·œ ê³ ê°")

    # ì±„ë„ë³„
    st.subheader("ğŸ“Œ ì±„ë„ë³„ ì„¸ì…˜ & ì‹ ê·œ ê³ ê°")
    channel_summary = (
        df.groupby("channelGrouping")
        .agg(sessions=("session_id", "nunique"), new_sessions=("totals_newVisits", "sum"))
        .reset_index()
        .sort_values("sessions", ascending=False)
    )
    channel_summary["new_visit_ratio"] = channel_summary["new_sessions"] / channel_summary["sessions"]

    fig, ax1 = plt.subplots(figsize=(8, 5))
    ax1.bar(channel_summary["channelGrouping"], channel_summary["sessions"])
    ax1.set_ylabel("Sessions")
    ax2 = ax1.twinx()
    ax2.plot(channel_summary["channelGrouping"], channel_summary["new_visit_ratio"], marker="o")
    ax2.set_ylabel("New Visit Ratio")
    plt.xticks(rotation=45)
    st.pyplot(fig)

    # ë””ë°”ì´ìŠ¤ë³„
    st.subheader("ğŸ“Œ ë””ë°”ì´ìŠ¤ë³„ ì„¸ì…˜ & ì‹ ê·œ ê³ ê°")
    device_summary = (
        df.groupby("device_deviceCategory")
        .agg(sessions=("session_id", "nunique"), new_sessions=("totals_newVisits", "sum"))
        .reset_index()
    )
    device_summary["new_visit_ratio"] = device_summary["new_sessions"] / device_summary["sessions"]

    fig, ax1 = plt.subplots(figsize=(6, 4))
    ax1.bar(device_summary["device_deviceCategory"], device_summary["sessions"])
    ax2 = ax1.twinx()
    ax2.plot(device_summary["device_deviceCategory"], device_summary["new_visit_ratio"], marker="o")
    plt.title("Device Category: Sessions & New Visit Ratio")
    st.pyplot(fig)

    # OSë³„ (Top 10)
    st.subheader("ğŸ“Œ OSë³„ ì„¸ì…˜ & ì‹ ê·œ ê³ ê°")
    os_summary = (
        df.groupby("device_operatingSystem")
        .agg(sessions=("session_id", "nunique"), new_sessions=("totals_newVisits", "sum"))
        .reset_index()
        .sort_values("sessions", ascending=False)
        .head(10)
    )
    os_summary["new_visit_ratio"] = os_summary["new_sessions"] / os_summary["sessions"]

    fig, ax1 = plt.subplots(figsize=(10, 5))
    ax1.bar(os_summary["device_operatingSystem"], os_summary["sessions"])
    ax2 = ax1.twinx()
    ax2.plot(os_summary["device_operatingSystem"], os_summary["new_visit_ratio"], marker="o")
    plt.xticks(rotation=45)
    plt.title("OS (Top 10): Sessions & New Visit Ratio")
    st.pyplot(fig)

    # Device Ã— Channel íˆíŠ¸ë§µ (í‰ê·  í˜ì´ì§€ë·°)
    st.subheader("ğŸ“Œ Device Ã— Channel íˆíŠ¸ë§µ (í‰ê·  í˜ì´ì§€ë·°)")
    pivot = df.pivot_table(
        index="device_deviceCategory", columns="channelGrouping", values="totals_pageviews", aggfunc="mean"
    )
    fig, ax = plt.subplots(figsize=(10, 5))
    sns.heatmap(pivot, annot=True, fmt=".1f", cmap="YlGnBu", ax=ax)
    plt.title("Average Pageviews by Device Ã— Channel")
    st.pyplot(fig)

    # 2ï¸âƒ£ ML ê¸°ë°˜ ê±°ë˜ ì˜ˆì¸¡
    st.header("2ï¸âƒ£ ML ê¸°ë°˜ ê±°ë˜ ì˜ˆì¸¡ (Logistic Regression)")
    st.markdown(
        "**ëª©í‘œ:** `channelGrouping`, `device_deviceCategory`, `device_operatingSystem`, "
        "`totals_pageviews`, `session_duration`, `totals_bounces` â†’ `is_transaction` ì˜ˆì¸¡"
    )

    features = [
        "channelGrouping",
        "device_deviceCategory",
        "device_operatingSystem",
        "totals_pageviews",
        "session_duration",
        "totals_bounces",
    ]
    X = df[features].copy()
    y = df["is_transaction"]

    numeric_features = ["totals_pageviews", "session_duration", "totals_bounces"]
    categorical_features = ["channelGrouping", "device_deviceCategory", "device_operatingSystem"]

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), numeric_features),
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
        ]
    )
    model = Pipeline(
        steps=[("preprocessor", preprocessor), ("classifier", LogisticRegression(max_iter=1000, class_weight="balanced"))]
    )

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    model.fit(X_train, y_train)

    st.write("Train Accuracy:", model.score(X_train, y_train))
    st.write("Test Accuracy:", model.score(X_test, y_test))
    y_proba = model.predict_proba(X_test)[:, 1]
    st.write("ROC AUC:", roc_auc_score(y_test, y_proba))

    # Feature Importance
    ohe = model.named_steps["preprocessor"].named_transformers_["cat"]
    cat_features = ohe.get_feature_names_out(categorical_features)
    all_features = numeric_features + list(cat_features)
    coef = model.named_steps["classifier"].coef_[0]
    importance = pd.DataFrame({"feature": all_features, "coef": coef}).sort_values("coef")

    st.subheader("ğŸ“ˆ Top Positive Factors (ê±°ë˜ í™•ë¥  â†‘)")
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.barplot(data=importance.tail(10), x="coef", y="feature", ax=ax)
    st.pyplot(fig)

    st.subheader("ğŸ“‰ Top Negative Factors (ê±°ë˜ í™•ë¥  â†“)")
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.barplot(data=importance.head(10), x="coef", y="feature", ax=ax)
    st.pyplot(fig)

    # 3ï¸âƒ£ ì±„ë„ ì§ˆ í‰ê°€ (íšŒê·€ê³„ìˆ˜ ê¸°ë°˜ ì ìˆ˜í™”)
    st.header("3ï¸âƒ£ ì±„ë„ ì§ˆ í‰ê°€ (íšŒê·€ê³„ìˆ˜ ê¸°ë°˜ ì ìˆ˜í™”)")
    # ê³„ìˆ˜ â†’ ê°€ì¤‘ì¹˜
    weights = {
        "avg_pageviews": abs(float(importance.loc[importance["feature"] == "totals_pageviews", "coef"].values[0] if "totals_pageviews" in importance["feature"].values else 1.0)),
        "avg_time_on_site": abs(float(importance.loc[importance["feature"] == "session_duration", "coef"].values[0] if "session_duration" in importance["feature"].values else 1.0)),
        "bounce_score": abs(float(importance.loc[importance["feature"] == "totals_bounces", "coef"].values[0] if "totals_bounces" in importance["feature"].values else 1.0)),
    }
    st.write(weights)

    quality_summary = (
        df.groupby("channelGrouping")
        .agg(
            avg_pageviews=("totals_pageviews", "mean"),
            bounce_rate=("totals_bounces", "mean"),
            avg_time_on_site=("session_duration", "mean"),
            transaction_rate=("is_transaction", "mean"),
        )
        .reset_index()
    )

    scaler = MinMaxScaler()
    quality_summary[["avg_pageviews", "avg_time_on_site", "transaction_rate"]] = scaler.fit_transform(
        quality_summary[["avg_pageviews", "avg_time_on_site", "transaction_rate"]]
    )
    quality_summary["bounce_score"] = 1 - MinMaxScaler().fit_transform(quality_summary[["bounce_rate"]])
    quality_summary["final_score"] = (
        quality_summary["avg_pageviews"] * weights["avg_pageviews"]
        + quality_summary["avg_time_on_site"] * weights["avg_time_on_site"]
        + quality_summary["bounce_score"] * weights["bounce_score"]
    ) / sum(weights.values())
    quality_summary = quality_summary.sort_values("final_score", ascending=False)

    st.dataframe(
        quality_summary[
            ["channelGrouping", "avg_pageviews", "bounce_score", "avg_time_on_site", "transaction_rate", "final_score"]
        ]
    )
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.barplot(data=quality_summary, x="final_score", y="channelGrouping", palette="Blues_r", ax=ax)
    ax.set_title("ì±„ë„ë³„ ìµœì¢… ì§ˆ í‰ê°€ ì ìˆ˜ (íšŒê·€ê³„ìˆ˜ ê°€ì¤‘ì¹˜ ê¸°ë°˜)")
    st.pyplot(fig)

    # 4ï¸âƒ£ ì±„ë„ë³„ ì‹ ê·œ vs ì¬ë°©ë¬¸ ë¹„ìœ¨
    st.header("4ï¸âƒ£ ì±„ë„ë³„ ì‹ ê·œ vs ì¬ë°©ë¬¸ ë¹„ìœ¨")
    channel_visits = (
        df.groupby("channelGrouping")["totals_newVisits"]
        .agg(sessions="count", new_sessions="sum")
        .reset_index()
        .sort_values("sessions", ascending=False)
    )
    channel_visits["repeat_sessions"] = channel_visits["sessions"] - channel_visits["new_sessions"]
    channel_visits["new_ratio"] = channel_visits["new_sessions"] / channel_visits["sessions"]
    channel_visits["repeat_ratio"] = channel_visits["repeat_sessions"] / channel_visits["sessions"]

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(channel_visits["channelGrouping"], channel_visits["new_ratio"], label="ì‹ ê·œ")
    ax.bar(
        channel_visits["channelGrouping"],
        channel_visits["repeat_ratio"],
        bottom=channel_visits["new_ratio"],
        label="ì¬ë°©ë¬¸",
    )
    ax.set_ylabel("ë¹„ìœ¨")
    ax.set_xlabel("ì±„ë„")
    plt.xticks(rotation=45)
    plt.legend()
    plt.title("ì±„ë„ë³„ ì‹ ê·œ vs ì¬ë°©ë¬¸ ë¹„ìœ¨")
    st.pyplot(fig)

    # ì¶”ì„¸ì„ (Top 4 ì±„ë„)
    aq = (
        sf.groupby("channel", observed=False)
        .agg(
            sessions=("session_id", "count"),
            new_ratio=("newVisit", "mean"),
            pv_mean=("pv", "mean"),
            bounce_rate=("bounces", "mean"),
            dur_mean=("session_duration", "mean"),
            conv_rate=("is_transaction", "mean"),
        )
        .sort_values("sessions", ascending=False)
        .reset_index()
    )
    st.markdown("**ì‹ ê·œ ë°©ë¬¸ ë¹„ìœ¨ ì¶”ì„¸ (Top 4 ì±„ë„)**")
    top4 = aq.head(4)["channel"].tolist()
    daily_new = (
        sf[sf["channel"].isin(top4)]
        .groupby(["session_date", "channel"], observed=False)
        .agg(new_ratio=("newVisit", "mean"), sessions=("session_id", "count"))
        .reset_index()
    )
    st.altair_chart(
        alt.Chart(daily_new).mark_line().encode(
            x="session_date:T", y=alt.Y("new_ratio:Q", axis=alt.Axis(format="~%")), color="channel:N"
        ).properties(height=280),
        use_container_width=True,
    )
    st.markdown("**ì„¸ì…˜ ìˆ˜ ì¶”ì„¸ (Top 4 ì±„ë„)**")
    st.altair_chart(
        alt.Chart(daily_new).mark_line().encode(x="session_date:T", y="sessions:Q", color="channel:N").properties(height=280),
        use_container_width=True,
    )

st.success("ì™„ë£Œ! ì‚¬ì´ë“œë°”ì—ì„œ ê¸°ê°„/ì±„ë„/ë””ë°”ì´ìŠ¤/Activation ê¸°ì¤€ì„ ë°”ê¿”ê°€ë©° Retention~Acquisitionì„ íƒìƒ‰í•˜ì„¸ìš”.")
